{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from decimal import Decimal\n",
    "from pyspark.sql import Row\n",
    "from math import log10 as log\n",
    "from pyspark.sql.functions import desc\n",
    "# from pyspark.context import SparkContext, SparkConf\n",
    "# from pyspark import StorageLevel\n",
    "# from pyspark import RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile('jane_austen/pride_and_prejudice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se genera un RDD con id del documento como clave y el contenido como valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = sc.wholeTextFiles('jane_austen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'\n",
    "stopwords= [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\" \n",
    "            , \"again\", \"against\", \"all\",\"almost\", \"alone\", \"along\", \"already\", \n",
    "            \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \n",
    "            \"amount\", \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\n",
    "            \"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\",\"back\",\"be\",\"became\",\n",
    "            \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \n",
    "            \"behind\", \"being\",\"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \n",
    "            \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\",\"cant\", \"co\", \"con\", \"could\", \n",
    "            \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"did\",\"do\", \"done\", \"down\", \"due\", \"during\",\"each\", \n",
    "            \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \n",
    "            \"ever\", \"every\",\"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \n",
    "            \"fill\", \"find\", \"fire\", \"first\",\"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \n",
    "            \"from\", \"front\", \"full\", \"further\", \"get\", \"give\",\"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \n",
    "            \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\",\"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "            \"how\", \"however\", \"hundred\", \"i\",\"ie\", \"if\", \"in\", \"inc\", \"indeed\",\"interest\", \"into\", \"is\", \"it\", \"its\", \n",
    "            \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\",\"made\", \"many\", \"may\", \"me\",\n",
    "            \"meanwhile\", \"might\", \"mill\", \"mine\", \"miss\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\",\"mr.\", \"mrs\",\"mrs.\",\"much\", \"must\", \"my\", \n",
    "            \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\",\"nobody\", \"none\",\n",
    "            \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\",\"only\", \n",
    "            \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\",\"per\", \n",
    "            \"perhaps\", \"please\", \"put\", \"rather\", \"re\",\"said\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\",\n",
    "            \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \n",
    "            \"somehow\",\"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\",\n",
    "            \"take\", \"ten\", \"than\",\"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \n",
    "            \"thereafter\", \"thereby\", \"therefore\",\"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \n",
    "            \"this\", \"those\", \"though\", \"three\", \"through\",\"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \n",
    "            \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\",\"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\",\n",
    "            \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\",\"when\", \"whence\", \"whenever\", \"where\", \"whereafter\",\n",
    "            \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\",\"whether\", \"which\", \"while\", \"whither\", \"who\",\n",
    "            \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\"within\", \"without\", \"would\", \"yet\", \n",
    "            \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", '', 'whatsoever', 'away', 'lady', 'ebook', \n",
    "            'www', 'online', 'org', 's', 'project', 'shall', 'sir', 'gutenberg', 'know', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words_from_book(b):\n",
    "    words = re.split(\"\\\\W+\", b.lower())\n",
    "    words = filter(lambda x : x not in stopwords, words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "books = books.map(lambda x: (x[1], x[0]))\n",
    "books = books.mapValues(lambda path: path.rsplit('/', 1)[-1])\n",
    "books = books.map(lambda x: (x[1], x[0]))\n",
    "D = books.count()\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_books = books.mapValues(lambda book: split_words_from_book(book))\n",
    "words_from_book = tokenized_books.flatMapValues(lambda x: x).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('emma.txt', 'emma10'), 1),\n",
       " (('emma.txt', 'jane'), 301),\n",
       " (('emma.txt', '1994'), 1),\n",
       " (('emma.txt', '158'), 1),\n",
       " (('emma.txt', 'public'), 14),\n",
       " (('emma.txt', 'domain'), 1),\n",
       " (('emma.txt', 'chapter'), 56),\n",
       " (('emma.txt', 'comfortable'), 34),\n",
       " (('emma.txt', 'home'), 132),\n",
       " (('emma.txt', 'unite'), 3)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frecuencies = words_from_book.map(lambda x: (x,1))\n",
    "term_frecuencies = term_frecuencies.reduceByKey(lambda v,w: v + w).persist()\n",
    "term_frecuencies.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emma.txt', 865),\n",
       " ('persuasion.txt', 497),\n",
       " ('lady_susan.txt', 110),\n",
       " ('mansfield_park.txt', 925),\n",
       " ('pride_and_prejudice.txt', 635),\n",
       " ('sense_and_sensibility.txt', 685),\n",
       " ('northanger abbey.txt', 487),\n",
       " ('love_and_friendship.txt', 80)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_frecuencies = term_frecuencies\n",
    "max_frecuencies = term_frecuencies.map(lambda x: (x[0][0], x[1])).reduceByKey(max)#.collect()\n",
    "max_frecuencies.take(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_terms(book_term):\n",
    "    title = book_term[0][0]\n",
    "    word = book_term[0][1]\n",
    "    count_value = book_term[1]\n",
    "    return Row(book=title,term=word,value=count_value)\n",
    "\n",
    "def organize_max_tuple(max_values):\n",
    "    title = max_values[0]\n",
    "    max_value = max_values[1]\n",
    "    return Row(book=title,value=max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_count_RDD = term_frecuencies.map(organize_terms)\n",
    "max_count_RDD = max_frecuencies.map(organize_max_values)\n",
    "\n",
    "term_count = sqlContext.createDataFrame(term_count_RDD).persist()\n",
    "max_count = sqlContext.createDataFrame(max_count_RDD).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+-------------------+\n",
      "|          book|        term|          frecuency|\n",
      "+--------------+------------+-------------------+\n",
      "|lady_susan.txt|        jane|0.03636363636363636|\n",
      "|lady_susan.txt|         use|0.12727272727272726|\n",
      "|lady_susan.txt|restrictions|0.01818181818181818|\n",
      "|lady_susan.txt|       title|0.01818181818181818|\n",
      "|lady_susan.txt|     posting|0.00909090909090909|\n",
      "+--------------+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf = term_count.join(max_count, term_count.book == max_count.book)\\\n",
    "                .select(term_count.book, \"term\", (term_count.value/max_count.value).alias(\"frecuency\"))\n",
    "\n",
    "tf_query = tf.select('book', 'term', 'frecuency')\n",
    "tf_query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emma10', 1),\n",
       " ('jane', 8),\n",
       " ('1994', 5),\n",
       " ('158', 1),\n",
       " ('public', 8),\n",
       " ('domain', 7),\n",
       " ('chapter', 6),\n",
       " ('comfortable', 8),\n",
       " ('home', 8),\n",
       " ('unite', 7)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frecuency_RDD = words_from_book.distinct().map(lambda x: (x[1],1)).reduceByKey(lambda v,w: v + w).persist()\n",
    "document_frecuency_RDD.take(10)\n",
    "# list(document_frecuency)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_idf(term):\n",
    "    term_name = term[0]\n",
    "    doc_count = term[1]\n",
    "    if(doc_count == 0):\n",
    "        term_idf = 0\n",
    "    else:\n",
    "        term_idf = log(D/doc_count)\n",
    "    return Row(term=term_name,value=term_idf)\n",
    "\n",
    "term_idf = document_frecuency_RDD.map(organize_doc_frecuency)\n",
    "idf = sqlContext.createDataFrame(term_idf).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+--------------------+\n",
      "|     term|           tf-idf|                book|\n",
      "+---------+-----------------+--------------------+\n",
      "|     dear|              8.0|love_and_friendsh...|\n",
      "|   father|              6.7|love_and_friendsh...|\n",
      "|   mother|              6.4|love_and_friendsh...|\n",
      "|   letter|              6.2|love_and_friendsh...|\n",
      "|     love|              6.0|love_and_friendsh...|\n",
      "|     anne|              6.0|      persuasion.txt|\n",
      "|     time|              5.9|love_and_friendsh...|\n",
      "|   having|              5.2|love_and_friendsh...|\n",
      "|catherine|              5.0|northanger abbey.txt|\n",
      "|elizabeth|              5.0|pride_and_prejudi...|\n",
      "|     work|              5.0|love_and_friendsh...|\n",
      "|  replied|              4.7|love_and_friendsh...|\n",
      "|    think|              4.7|love_and_friendsh...|\n",
      "|   mother|4.581818181818182|      lady_susan.txt|\n",
      "|    great|              4.3|love_and_friendsh...|\n",
      "|     dear|4.218181818181818|      lady_susan.txt|\n",
      "|    young|              4.1|love_and_friendsh...|\n",
      "|   edward|4.050000000000001|love_and_friendsh...|\n",
      "|    fanny|              4.0|  mansfield_park.txt|\n",
      "|      say|              4.0|love_and_friendsh...|\n",
      "+---------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tf.join(idf, tf.term == idf.term).select(tf.term, (tf.frecuency*idf.value).alias('tf-idf'), tf.book)\n",
    "tf_idf_query = tf_idf.select('term', 'tf-idf', 'book').orderBy(desc('tf-idf'))\n",
    "tf_idf_query.show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark2",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
